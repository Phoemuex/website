<!DOCTYPE html>

<html>

    <head>

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <title> Felix Voigtlaender </title>

        <link rel="stylesheet" media="screen" href="bootstrap.min.css">

    </head>

    <body>

        <nav class="navbar navbar-light bg-faded">

            <div class="container">

                <ul class="nav navbar-nav">

                    <li class="nav-item"> <a class="navbar-brand active" href="index.html"> Home </a> </li>

                    <li class="nav-item"> <a class="navbar-brand" href="publications.html"> Publications </a> </li>

                    <li class="nav-item"> <a class="navbar-brand active" href="impressum.html"> Impressum </a> </li>

                </ul>

            </div>

        </nav>

        <div class="container">

            <hr>

            <div class="row">

                <div class="col-xs-4 col-sm-3">
                    <a href="felix_new.jpg">
                        <img src="felix_new.jpg" class="img-fluid img-rounded" alt="Responsive image">
                    </a>
                    <div class="small">
                        Photograph by <a href="http://lutzvoigtlaender.com/">Lutz Voigtlaender</a>
                    </div>
                </div>

                <div class="col-xs-8 col-sm-9">

                    <h3> Felix Voigtlaender </h3>

                    Senior scientist in mathematics<br>

                    in the group of <a href="https://mat.univie.ac.at/~grohs/">Philipp Grohs</a><br>

                    <a href="https://mathematik.univie.ac.at/en/">University of Vienna</a><br>

                    <br>

                    Email: felix@voigtlaender.xyz <br>

                    <a href="cv.pdf"> CV </a> (last updated on 23 May 2020)

                    <br>
                    <br>
                    <br>

                    <blockquote>
                        <p class="lead">If you trust in yourself. . .and believe in your dreams. . .and follow your star. . . you'll still get beaten by people who spent their time working hard and learning things and weren't so lazy.</p>
                        <foother>&mdash;Terry Pratchett, The Wee Free Men</footer>
                    </blockquote>

                </div>

            </div>


            <hr>

            <div class="row">

                <div class="col-xs-12">

                    <h3> Research interests </h3>

                    I am interested in Functional analysis, Harmonic analysis, and the mathematics of machine learning.
                    More precisely, my main interests are
                    <ul>
                        <li>Function spaces and their embeddings</li>
                        <li>Multiscale systems (wavelets, curvelets, shearlets and generalizations)</li>
                        <li>Banach frames and atomic decompositions</li>
                        <li>Sampling</li>
                        <li>Neural networks and their properties, in particular their expressiveness and approximation properties</li>
                        <li>Coorbit theory</li>
                        <li>Approximation theory</li>
                        <li>Time-frequency analysis</li>
                        <li>Fourier analysis</li>
                    </ul>

                    As a mathematical hobby, I am interested in almost everything related to measure theory and in trying to crack toy problems
                    like <a href="http://math.stackexchange.com/questions/1552990/is-it-possible-to-characterize-completeness-of-a-normed-vector-space-by-converge">this one</a> (which is solved by now).
                </div>

            </div>

            <hr>

            <div class="row">

                <div class="col-xs-12">

                    <h3> Biography </h3>

                    <p> I am currently a Senior Scientist in the <a href="https://mathematik.univie.ac.at/en/">Department of Mathematics</a> at the University of Vienna, part of the group of <a href="https://mat.univie.ac.at/~grohs/">Philipp Grohs</a>.</p>

                    <p> I studied mathematics (Bachelor + Master) and computer science (Bachelor) at <a href="http://www.rwth-aachen.de/">RWTH Aachen University</a>, Germany, where I graduated with the Master degree with distinction in 2013.</p>

                    <p>As a Ph.D. student, I then joined the group of <a href="http://www.matha.rwth-aachen.de/en/mitarbeiter/fuehr/">Hartmut Führ</a> at RWTH Aachen University, where I studied the approximation theoretic properties of different multiscale systems.
                    With my Ph.D. thesis <em>'Embedding Theorems for Decomposition Spaces with applications to Wavelet Coorbit Spaces'</em>, I graduated with distinction in November 2015.
                    In November 2016, I was awarded the <a href="http://www.mathematik.rwth-aachen.de/cms/Mathematik/Forschung/Auszeichnungen/~gskt/Friedrich-Wilhelm-Preis/?lidx=1">Friedrich-Wilhelm-Award 2016</a>
                    for my thesis.
                    </p>

                    <p> I highly enjoy teaching mathematics and am committed to explaining carefully and presenting the material in an enjoyable way.
                    At RWTH Aachen University, I have been teaching assistant for Analysis I and III, and for Harmonic analysis.
                    Therefore, I am very proud to have received the <a href="https://www.mathematik.rwth-aachen.de/cms/Mathematik/Forschung/Auszeichnungen/~gslc/AbsolventenTag-Mathematik-Lehrpreise/">Teaching award of the student council of mathematics at RWTH Aachen University</a>.
                    </p>

                    <p>From April 2016 until January 2018, I was a postdoctoral researcher at TU Berlin in the group of <a href="https://scholar.google.de/citations?user=JHs9LssAAAAJ&hl=en">Gitta Kutyniok</a>, where I worked on the <a href="http://dedale.cosmostat.org/">DEDALE</a> project.
                    As part of that project, and with support by <a href="http://annepein.com/">Anne Pein</a>, I developed the <a href="https://github.com/dedale-fet/alpha-transform">DEDALE α-shearlet transform</a>.
                    </p>

                    <p>From February 2018 to May 2020, I worked as an "Akademischer Rat" as part of the <a href="https://www.ku.de/en/mgf/mathematics/scientific-computing/team-of-the-department-of-scientific-computing">Scientific Computing Group</a> at KU Eichstätt, lead by <a href="https://www.ku.de/en/mgf/mathematics/scientific-computing/team-of-the-department-of-scientific-computing/prof-dr-goetz-pfander#c4691">Götz Pfander</a>.</p>
                </div>

            </div>

            <hr>

            <div class="row">

                <div class="col-xs-12">

                    <h3> News </h3>

                    <div class="row">
                        <div class="col-xs-12 col-sm-3 text-sm-right"> <strong> 19 January 2021 </strong> </div>
                        <div class="col-xs-12 col-sm-9">
                            <p>Update of the website.
                            Added two papers.
                            In the <a href="https://arxiv.org/abs/2011.09363">first one</a>,
                            <a href="https://www.ku.de/mgf/mathematik/wissenschaftliches-rechnen/personen-des-lehrstuhls/andrei-caragea">Andrei Caragea</a>,
                            <a href="http://www.pc-petersen.eu/">Philipp Petersen</a>,
                            and I study the performance of neural networks
                            for high-dimensional classification problems with structured class boundaries.
                            In a nutshell, we show that if these boundaries are locally of Barron-type,
                            then one obtains learning and approximation bounds with rates independent of the underlying dimension.
                            <br>
                            In the <a href="https://arxiv.org/abs/2012.03351">second one</a>,
                            I generalize the classical <a href="https://doi.org/10.1016/S0893-6080(05)80131-5">universal approximation theorem</a>
                            to the setting of complex-valued neural networks.
                            Under very mild continuity assumptions on the activation function, I show for shallow networks that universality holds
                            if and only if the real- or the imaginary part of the activation function is not polyharmonic.
                            For networks with at least two hidden layers, universality holds if and only if the activation function
                            is neither a polynomial, nor holomorphic, nor antiholomorphic.
                            <br>
                            Added two talks, one of which is available on <a href="https://www.youtube.com/watch?v=daozFxwiD5s">Youtube</a>.<br>
                            Continued the endless fight against <a href="https://en.wikipedia.org/wiki/Link_rot">link rot</a>.
                            </p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-xs-12 col-sm-3 text-sm-right"> <strong> 23 May 2020 </strong> </div>
                        <div class="col-xs-12 col-sm-9">
                            <p>Update of the website and of the CV. Added several new preprints and talks. Updated photograph.</p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-xs-12 col-sm-3 text-sm-right"> <strong> 15 September 2018 </strong> </div>
                        <div class="col-xs-12 col-sm-9">
                            <p>Update of the website and of the CV. Added several new preprints and talks.</p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-xs-12 col-sm-3 text-sm-right"> <strong> 8 April 2018 </strong> </div>
                        <div class="col-xs-12 col-sm-9">
                            <p>First update of the website since I moved from Berlin to Eichstätt. The CV is still out of date, however...</p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-xs-12 col-sm-3 text-sm-right"> <strong> 23 November 2017 </strong> </div>
                        <div class="col-xs-12 col-sm-9">
                            <p>After my talk about the approximation properties of ReLU neural networks at the <a href="https://ins.uni-bonn.de/group/burstedde/page/seminar">Research seminar "Mathematics of Computation"</a>, I was asked for the slides to my talk. These can be found <a href="DNNBonnHandout.pdf">here</a>.</p>
                            <p>Many thanks to <a href="https://www.tu-chemnitz.de/mathematik/ang_analysis/index.php.en">Tino Ullrich</a> for inviting me to give this talk!</p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-xs-12 col-sm-3 text-sm-right"> <strong> 23 September 2017 </strong> </div>
                        <div class="col-xs-12 col-sm-9">
                            <p>I just added to my list of publications two <a href="https://arxiv.org/abs/1710.03576">new</a> <a href="https://arxiv.org/abs/1709.05289">preprints</a> that I recently uploaded to the arXiv.</p>
                            <p>
                            The first one establishes a rather general version of <a href="http://mathworld.wolfram.com/PricesTheorem.html">Price's theorem</a>: It gives a simple formula
                            for computing the partial derivatives of the map ρ ↦ 𝔼[g(Xᵨ)], where Xᵨ is a normally distributed random variable with covariance matrix ρ. Price published
                            this result in 1958, but did not precisely state the required assumptions on g. In <a href="https://arxiv.org/abs/1710.03576">the paper</a>, I show that one
                            can in fact take every tempered distribution g.
                            This result is used in the paper <a href="https://arxiv.org/abs/1710.04952">ℓ¹-Analysis Minimization and Generalized (Co-)Sparsity: When Does Recovery Succeed?</a>
                            written by three of my colleagues.
                            </p>

                            <p>
                            The <a href="https://arxiv.org/abs/1709.05289">second preprint</a>, written jointly with my colleague
                            <a href="http://www.pc-petersen.eu/index.html">Philipp Petersen</a>
                            analyzes the approximation power of neural networks that use the ReLU activation function.
                            We analyze how deep and wide such a neural network needs to be, in order
                            to approximate any "piecewise smooth" function. We also show that these bounds are sharp.
                            </p>

                            <p>
                            I also added several talks that I gave in the last months, including the
                            <a href="http://anarm.dima.unige.it/genova2017/files/voigtlaender_notes.pdf">lecture notes</a>
                            for the lecture series "Sparsity Properties of Frames via Decomposition Spaces"
                            that I gave at the <a href="http://anarm.dima.unige.it/genova2017/">Summer School on Applied Harmonic Analysis</a> in Genoa.
                            </p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-xs-12 col-sm-3 text-sm-right"> <strong> 09 February 2017 </strong> </div>
                        <div class="col-xs-12 col-sm-9">
                            <p>
                            I just added a new <a href="https://arxiv.org/abs/1612.08772">preprint</a> to my list of publications that I uploaded to the arXiv in December. <br>
                            </p>
                            <p>
                            Furthermore, I am <i>very</i> happy to be a speaker at the <a href="http://anarm.dima.unige.it/genova2017/">Summer School on Applied Harmonic Analysis</a> that will take place in Genoa from July 24-28, 2017.<br>
                            </p>
                            <p>
                            Finally, I want to mention that my <a href="http://math.stackexchange.com/questions/1552990/is-it-possible-to-characterize-completeness-of-a-normed-vector-space-by-converge">toy problem</a>
                            (about whether completeness of spaces can be characterized by the convergence of Neumann series) has been solved (negatively) quite a while ago.
                            </p>
                        </div>
                    </div>

                    <div class="row">
                        <div class="col-xs-12 col-sm-3 text-sm-right"> <strong> 28 July 2016 </strong> </div>
                        <div class="col-xs-12 col-sm-9"> <p>First version of this website was uploaded :) </p> </div>
                    </div>

                </div>

            </div>

        </div>

    </body>

</html>
